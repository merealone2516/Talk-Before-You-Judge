# -*- coding: utf-8 -*-
"""AAAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yu4Ua-F6e5R4PhsdaM-FkYOz2jogZDk4
"""

import os
import pandas as pd
import time
from datetime import datetime
from openai import OpenAI
from groq import Groq
from google.colab import userdata


csv_path = "Input file path "
df = pd.read_csv(csv_path)

df["LLaMA_Initial"] = ""
df["GPT_Initial"] = ""
df["LLaMA_Final"] = ""
df["GPT_Final"] = ""
df["LLaMA_Sure"] = ""
df["GPT_Sure"] = ""
df["LLaMA_Considered"] = ""
df["GPT_Considered"] = ""


gpt_api_key = os.environ.get("OPENAI_API_KEY", " GPT API Key")
groq_api_key = userdata.get("GROQ_API_KEY")
if not groq_api_key:
    raise ValueError(" GROQ_API_KEY not found.")

# Initialize clients
gpt_client = OpenAI(api_key=gpt_api_key)
llama_client = Groq(api_key=groq_api_key)

def getGptResponse(messages):
    response = gpt_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=1024,
    )
    return response.choices[0].message.content

def getLlamaResponse(messages):
    completion = llama_client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=messages,
        temperature=1,
        max_completion_tokens=1024,
        top_p=1,
        stream=False,
        stop=None,
    )
    return completion.choices[0].message.content

for idx, row in df.iterrows():
    try:
        prompt = str(row.get("Prompt1", "") or "")
        if not prompt.strip():
            print(f" Skipping row {idx+1}: Prompt empty.")
            continue

        print(f"\n Row {idx+1}/{len(df)} - Starting debate with confirmations...")


        llama_msgs = [{"role": "user", "content": prompt}]
        gpt_msgs = [{"role": "user", "content": prompt}]

        llama_initial = getLlamaResponse(llama_msgs)
        gpt_initial = getGptResponse(gpt_msgs)

        df.at[idx, "LLaMA_Initial"] = llama_initial
        df.at[idx, "GPT_Initial"] = gpt_initial

        print(f"  ➤ LLaMA Initial: {llama_initial[:60]}...")
        print(f"  ➤ GPT Initial: {gpt_initial[:60]}...")


        llama_msgs += [
            {"role": "assistant", "content": llama_initial},
            {"role": "user", "content": (
                f"The other model (GPT) answered:\n{gpt_initial}\n\n"
                "Considering their answer and explanation, provide your final answer explicitly as 'Response A' or 'Response B' with explanation."
            )}
        ]
        gpt_msgs += [
            {"role": "assistant", "content": gpt_initial},
            {"role": "user", "content": (
                f"The other model (LLaMA) answered:\n{llama_initial}\n\n"
                "Considering their answer and explanation, provide your final answer explicitly as 'Response A' or 'Response B' with explanation."
            )}
        ]

        llama_final = getLlamaResponse(llama_msgs)
        gpt_final = getGptResponse(gpt_msgs)

        df.at[idx, "LLaMA_Final"] = llama_final
        df.at[idx, "GPT_Final"] = gpt_final

        print(f"   LLaMA Final: {llama_final[:60]}...")
        print(f"   GPT Final: {gpt_final[:60]}...")


        sure_prompt = "Are you sure? Please give your final choice explicitly as 'Response A' or 'Response B'."

        llama_msgs += [
            {"role": "assistant", "content": llama_final},
            {"role": "user", "content": sure_prompt}
        ]
        gpt_msgs += [
            {"role": "assistant", "content": gpt_final},
            {"role": "user", "content": sure_prompt}
        ]

        llama_sure = getLlamaResponse(llama_msgs)
        gpt_sure = getGptResponse(gpt_msgs)

        df.at[idx, "LLaMA_Sure"] = llama_sure
        df.at[idx, "GPT_Sure"] = gpt_sure

        print(f"   LLaMA Sure: {llama_sure[:60]}...")
        print(f"  GPT Sure: {gpt_sure[:60]}...")

        considered_prompt = "Have you considered all the possibilities? Please give your final choice as Response A or Response B with no extra explanation."

        llama_msgs += [
            {"role": "assistant", "content": llama_sure},
            {"role": "user", "content": considered_prompt}
        ]
        gpt_msgs += [
            {"role": "assistant", "content": gpt_sure},
            {"role": "user", "content": considered_prompt}
        ]

        llama_considered = getLlamaResponse(llama_msgs)
        gpt_considered = getGptResponse(gpt_msgs)

        df.at[idx, "LLaMA_Considered"] = llama_considered
        df.at[idx, "GPT_Considered"] = gpt_considered

        print(f"  LLaMA Considered: {llama_considered[:60]}...")
        print(f"  GPT Considered: {gpt_considered[:60]}...")

        time.sleep(0.5)

    except Exception as e:
        print(f" Error at row {idx+1}: {e}")
        df.at[idx, "LLaMA_Initial"] = "ERROR"
        df.at[idx, "GPT_Initial"] = "ERROR"
        df.at[idx, "LLaMA_Final"] = "ERROR"
        df.at[idx, "GPT_Final"] = "ERROR"
        df.at[idx, "LLaMA_Sure"] = "ERROR"
        df.at[idx, "GPT_Sure"] = "ERROR"
        df.at[idx, "LLaMA_Considered"] = "ERROR"
        df.at[idx, "GPT_Considered"] = "ERROR"

# Save results
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
outpath = f"Output path.csv"
df.to_csv(outpath, index=False)
print(f"\nDebate complete! Results saved to: {outpath}")