# -*- coding: utf-8 -*-
"""AAAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yu4Ua-F6e5R4PhsdaM-FkYOz2jogZDk4
"""

import os
import pandas as pd
import time
from datetime import datetime
from openai import OpenAI
from groq import Groq
from google.colab import userdata, drive


csv_path = "Input file path "
df = pd.read_csv(csv_path)


df["LLaMA_Initial"] = ""
df["DeepSeek_Initial"] = ""
df["LLaMA_Final"] = ""
df["DeepSeek_Final"] = ""
df["LLaMA_Sure"] = ""
df["DeepSeek_Sure"] = ""
df["LLaMA_Considered"] = ""
df["DeepSeek_Considered"] = ""


deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY", "Deepseek Key")
groq_api_key = userdata.get("GROQ_API_KEY")
if not groq_api_key:
    raise ValueError(" GROQ_API_KEY not found.")

deepseek_client = OpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com")
llama_client = Groq(api_key=groq_api_key)

def getDeepSeekResponse(messages):
    response = deepseek_client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        max_tokens=1024,
    )
    return response.choices[0].message.content

def getLlamaResponse(messages):
    completion = llama_client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=messages,
        temperature=1,
        max_completion_tokens=1024,
        top_p=1,
        stream=False,
        stop=None,
    )
    return completion.choices[0].message.content

for idx, row in df.iterrows():
    try:
        prompt = str(row.get("Prompt1", "") or "")
        if not prompt.strip():
            print(f" Skipping row {idx+1}: Prompt empty.")
            continue

        print(f"\n Row {idx+1}/{len(df)} - Starting LLaMA + DeepSeek conversation...")


        llama_msgs = [{"role": "user", "content": prompt}]
        deepseek_msgs = [{"role": "user", "content": prompt}]

        llama_initial = getLlamaResponse(llama_msgs)
        deepseek_initial = getDeepSeekResponse(deepseek_msgs)

        df.at[idx, "LLaMA_Initial"] = llama_initial
        df.at[idx, "DeepSeek_Initial"] = deepseek_initial

        print(f"  âž¤ LLaMA Initial: {llama_initial[:60]}...")
        print(f"  âž¤ DeepSeek Initial: {deepseek_initial[:60]}...")


        llama_msgs += [
            {"role": "assistant", "content": llama_initial},
            {"role": "user", "content": (
                f"The other model (DeepSeek) answered:\n{deepseek_initial}\n\n"
                "Considering their answer and explanation, provide your final answer explicitly as 'Response A' or 'Response B' with explanation."
            )}
        ]
        deepseek_msgs += [
            {"role": "assistant", "content": deepseek_initial},
            {"role": "user", "content": (
                f"The other model (LLaMA) answered:\n{llama_initial}\n\n"
                "Considering their answer and explanation, provide your final answer explicitly as 'Response A' or 'Response B' with explanation."
            )}
        ]

        llama_final = getLlamaResponse(llama_msgs)
        deepseek_final = getDeepSeekResponse(deepseek_msgs)

        df.at[idx, "LLaMA_Final"] = llama_final
        df.at[idx, "DeepSeek_Final"] = deepseek_final

        print(f"   LLaMA Final: {llama_final[:60]}...")
        print(f"   DeepSeek Final: {deepseek_final[:60]}...")

        sure_prompt = "Are you sure? Please give your final choice explicitly as 'Response A' or 'Response B'."

        llama_msgs += [
            {"role": "assistant", "content": llama_final},
            {"role": "user", "content": sure_prompt}
        ]
        deepseek_msgs += [
            {"role": "assistant", "content": deepseek_final},
            {"role": "user", "content": sure_prompt}
        ]

        llama_sure = getLlamaResponse(llama_msgs)
        deepseek_sure = getDeepSeekResponse(deepseek_msgs)

        df.at[idx, "LLaMA_Sure"] = llama_sure
        df.at[idx, "DeepSeek_Sure"] = deepseek_sure

        print(f"  ðŸ”Ž LLaMA Sure: {llama_sure[:60]}...")
        print(f"  ðŸ”Ž DeepSeek Sure: {deepseek_sure[:60]}...")


        considered_prompt = "Have you considered all the possibilities? Please give your final choice as Response A or Response B with no extra explanation."

        llama_msgs += [
            {"role": "assistant", "content": llama_sure},
            {"role": "user", "content": considered_prompt}
        ]
        deepseek_msgs += [
            {"role": "assistant", "content": deepseek_sure},
            {"role": "user", "content": considered_prompt}
        ]

        llama_considered = getLlamaResponse(llama_msgs)
        deepseek_considered = getDeepSeekResponse(deepseek_msgs)

        df.at[idx, "LLaMA_Considered"] = llama_considered
        df.at[idx, "DeepSeek_Considered"] = deepseek_considered

        print(f"   LLaMA Considered: {llama_considered[:60]}...")
        print(f"   DeepSeek Considered: {deepseek_considered[:60]}...")

        time.sleep(0.5)

    except Exception as e:
        print(f" Error at row {idx+1}: {e}")
        df.at[idx, "LLaMA_Initial"] = "ERROR"
        df.at[idx, "DeepSeek_Initial"] = "ERROR"
        df.at[idx, "LLaMA_Final"] = "ERROR"
        df.at[idx, "DeepSeek_Final"] = "ERROR"
        df.at[idx, "LLaMA_Sure"] = "ERROR"
        df.at[idx, "DeepSeek_Sure"] = "ERROR"
        df.at[idx, "LLaMA_Considered"] = "ERROR"
        df.at[idx, "DeepSeek_Considered"] = "ERROR"

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
outpath = f"Output File Path"
df.to_csv(outpath, index=False)