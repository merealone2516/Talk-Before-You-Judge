Current LLM-based evaluation systems often exhibit judgment instability, frequently changing their responses when prompted with follow-up queries that introduce no new information, such as “Are you sure?” To address this, we propose DUET (Deliberative Understanding for Evaluation Tasks) framework in which two large language models (LLMs) independently assess a task, exchange explanations, and revise their judgments through a single round of deliberation to improve decision stability. We systematically study response flipping across five prominent LLMs on 620 prompts spanning knowledge, mathematics, coding, and reasoning tasks. In addition to reducing flip rates, our approach significantly improves response stability, ensuring that models maintain consistent decisions under minimal challenge. Overall, our framework offers a dependable alternative to single-model evaluation and contributes to more trustworthy automated assessments.
